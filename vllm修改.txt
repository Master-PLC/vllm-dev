#############################  1
文件：
vllm/v1/outputs.py

源头：
@dataclass
class SamplerOutput:

动作：
    logprobs_tensors: Optional[LogprobsTensors]
的正下方添加一行
    logits_tensors: Optional[torch.Tensor]



源头：
@dataclass
class ModelRunnerOutput:

动作：
    # [num_reqs, max_num_logprobs + 1]
    # [num_reqs, max_num_logprobs + 1]
    # [num_reqs]
    logprobs: Optional[LogprobsLists]
的正下方添加

    # [num_reqs, vocab_size]
    logits: Optional[list[list[float]]]


EMPTY_MODEL_RUNNER_OUTPUT = ModelRunnerOutput(
    req_ids=[],
    req_id_to_index={},
    sampled_token_ids=[],
    spec_token_ids=None,
    logprobs=None,
    prompt_logprobs_dict={},
)
的
    logprobs=None,
正下方添加
    logits=None,



############################

vllm/v1/sample/metadata.py

    max_num_logprobs: Optional[int]
正下方添加
    return_logits: bool


#############################  2
文件：
vllm/v1/sample/sampler.py

源头：
class Sampler(nn.Module):

    def forward(
        self,
        logits: torch.Tensor,
        sampling_metadata: SamplingMetadata,
    ) -> SamplerOutput:

动作：
            logprobs_tensors=logprobs_tensors,
的正下方添加一行
            logits_tensors=logits if sampling_metadata.return_logits else None,


########################3
vllm/v1/worker/gpu_model_runner.py

        # NOTE: GPU -> CPU Sync happens here.
        # Move as many CPU operations as possible before this sync point.
        logprobs_tensors = sampler_output.logprobs_tensors
        logprobs_lists = logprobs_tensors.tolists() \
            if logprobs_tensors is not None else None
正下方添加

        logits_tensors = sampler_output.logits_tensors
        logits_lists = logits_tensors.tolist() \
            if logits_tensors is not None else None


        return ModelRunnerOutput(
            req_ids=self.input_batch.req_ids,
            req_id_to_index=self.input_batch.req_id_to_index,
            sampled_token_ids=valid_sampled_token_ids,
            spec_token_ids=spec_token_ids,
            logprobs=logprobs_lists,
            prompt_logprobs_dict=prompt_logprobs_dict,
        )
中的
            logprobs=logprobs_lists,
正下方添加
            logits=logits_lists,


        dummy_metadata = SamplingMetadata(
            temperature=dummy_tensors(0.5),
            all_greedy=False,
            all_random=False,
            top_p=dummy_tensors(0.9),
            top_k=dummy_tensors(logits.size(1) - 1),
            min_p=None,
            generators={},
            max_num_logprobs=None,
            no_penalties=True,
            prompt_token_ids=None,
            frequency_penalties=dummy_tensors(0.1),
            presence_penalties=dummy_tensors(0.1),
            repetition_penalties=dummy_tensors(0.1),
            output_token_ids=[[] for _ in range(num_reqs)],
            min_tokens={},
            logit_bias=[None for _ in range(num_reqs)],
            allowed_token_ids_mask=None,
            bad_words_token_ids={},
        )
的
            max_num_logprobs=None,
正下方添加
            return_logits=False,


#############################  2
文件：
vllm/v1/worker/gpu_input_batch.py

源头：
class InputBatch:

    def __init__(
        self,
        max_num_reqs: int,
        max_model_len: int,
        max_num_blocks_per_req: int,
        device: torch.device,
        pin_memory: bool,
        vocab_size: int,
    ):

动作：
        self.num_logprobs: dict[str, int] = {}
的正下方添加一行
        self.require_logits: dict[str, bool] = {}


源头：
    def add_request(
        self,
        request: "CachedRequestState",
        req_index: Optional[int] = None,
    ) -> None:

动作：
        if sampling_params.logprobs is not None:
            self.num_logprobs[req_id] = sampling_params.logprobs
下面添加
        self.require_logits[req_id] = sampling_params.return_logits

        self.num_logprobs.pop(req_id, None)
下面添加
        self.require_logits.pop(req_id, False)

            max_num_logprobs=self.max_num_logprobs,
下面添加
            return_logits=self.return_logits,

    @property
    def max_num_logprobs(self) -> Optional[int]:
        return max(self.num_logprobs.values()) if self.num_logprobs else None
下面添加
    @property
    def return_logits(self) -> bool:
        return any(self.require_logits.values())



#####################################
vllm/sampling_params.py

    logprobs: Optional[int] = None
下面添加
    return_logits: bool = False

        logprobs: Optional[int] = None,
下面添加
        return_logits: bool = False,


            logprobs=logprobs,
下面添加
            return_logits=return_logits,


            f"logprobs={self.logprobs}, "
下面添加
            f"return_logits={self.return_logits}, "





##################################

vllm/v1/core/sched/scheduler.py


    def update_from_output(
        self,
        scheduler_output: SchedulerOutput,
        model_runner_output: ModelRunnerOutput,
    ) -> EngineCoreOutputs:

        logprobs = model_runner_output.logprobs
正下方添加
        logits = model_runner_output.logits



            stopped = False
            new_logprobs = None
            new_token_ids = generated_token_ids
中的
            new_logprobs = None
下面添加
            new_logits = None



            # Extract sample logprobs if needed.
            if request.sampling_params.logprobs is not None and logprobs:
                # NOTE: once we support N tokens per step (spec decode),
                # the outer lists can be of length > 1.
                new_logprobs = logprobs.slice(req_index, req_index + 1)
下面添加
            if request.sampling_params.return_logits and logits:
                new_logits = logits[req_index:req_index + 1]


                outputs.append(
                    EngineCoreOutput(
                        request_id=req_id,
                        new_token_ids=new_token_ids,
                        finish_reason=request.get_finished_reason(),
                        new_logprobs=new_logprobs,
                        new_prompt_logprobs_tensors=prompt_logprobs_tensors,
                        stop_reason=request.stop_reason,
                        events=request.take_events()))
中的
                        new_logprobs=new_logprobs,
下面添加




#################################
vllm/v1/engine/__init__.py


class EngineCoreOutput(
        msgspec.Struct,
        array_like=True,  # type: ignore[call-arg]
        omit_defaults=True,  # type: ignore[call-arg]
        gc=False):  # type: ignore[call-arg]



    new_logprobs: Optional[LogprobsLists] = None
下面添加
    new_logits: Optional[list[list[float]]] = None